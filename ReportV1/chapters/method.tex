\chapter{Methodology}\label{ch:method}
In order to address the underlying research question in going 'beyond pure network infrastructure testing', this report formulates a testing methodology based on a set of use cases that span all layers from basic bandwidth saturation (on layers two and tree) up to the application layer (layer 7).
These use cases then lead to the selection of the correct testing application that fits the framework, bearing in mind the outcome of the per-tool experiments and the intended intensity level for each of the use cases.

\section{Use cases}\label{sec:usecase}

Table \ref{tab:usecases} displays four different use cases, all serving a specific purpose.
Use case 1 is designed to find the hardware limits of the NIC and the host. Use case 2 is to find the limits of the  switching and routing hardware of the DUT.
Use case 3 will find the client and server limits up to layer 4. To handle TCP sessions, resources need to be allocated at client and server. 
Use case 4 is designed to find the limits of an application running on top of a kernel.  
Use cases 1 and 2 will show local limitations. 
Use cases 3 and 4 are done from a client to a server over a corporate network, that includes a stateful firewall, aggregated links and active-standby machines in the path. The latter two cases will stress the weakest link in the infrastructure towards the server.  
It should be noted that monitoring is a necessary part of the test in order to determine which link in the path is the 'weakest'.

\begin{table}[]
\centering
\caption{Use cases}
\label{tab:usecases}
\begin{tabular}{|l|l|l|l|}
\hline
NR & Use case                        & DUT            & Goal                                                                                             \\ \hline
UC1  & \begin{tabular}[c]{@{}l@{}}Bandwidth \\ generation\end{tabular}       & Client         & \begin{tabular}[c]{@{}l@{}}The goal is to see if the client is capable \\ of filling up the link and to reach the \\ maximum amount of pps \end{tabular} \\ \hline
UC2  & Throughput                    & Switch/router  & \begin{tabular}[c]{@{}l@{}}Generate the maximum amount of data in 2 ways \\ to make sure the hardware is able to forward\\ at line rate\end{tabular} \\ \hline
UC3  & TCP based                     & Client/Server  & Get the hardware limitations of the systems  \\ \hline
UC4  & Application             & \begin{tabular}[c]{@{}l@{}}Server and \\intermediate devices\end{tabular}         & \begin{tabular}[c]{@{}l@{}}The clients will overload the server with \\ requests at application level\end{tabular}  \\ \hline
\end{tabular}
\end{table}

\subsection{UC1 Bandwidth Generation}
In paragraph \ref{par:pps} it is stated that a 40Gb/s card needs to be able to handle 56 million packets with a size of 88 byte to handle a unidirectional stream of 40Gb/s.
Using pktgen on top of DPDK, Ethernet frames with a minimum size of 64 bytes containing TCP traffic (without inter frame gap, preamble and VLAN tag) can be created. To reach the 40Gb/s, larger packets have to be created. When the packet size is set to 1500 bytes, there have to be 3.3 Mpps to saturate the link. When 9000 byte packets are used only 555 Kpps are needed to saturate the link. When using pktgen on top of DPDK numbers like these can be reached using TCP traffic. When DPDK is used, the kernel does not communicate with the devices anymore since the nature of DPDK is to bypass the kernel drivers because the interfaces are claimed by DPDK. So traffic statics cannot be read from the kernel. Monitoring on the switch ports connecting the servers is needed. 

\subsection{UC2 Throughput}
The backplane of a switch should be able to forward traffic from one port to another at line speed. This should also be possible when the traffic is routed from one VLAN to the other. Routers should be able to route packets at line rate. To test this a client and a server need to be connected to two different ports of the router in a different segment or VLAN and a high load should be generated from the client to the server, preferably the maximum link speed. Monitoring the ports of the switch using SNMP should show the same input rate on port 1 compared to the output rate of port 2.    

\subsection{UC3 TCP Based}
To determine the limitations of the server hardware when TCP is used, the kernel has to be bypassed. The combination of DPDK and WARP will find the limits of the hardware under load. As seen in the experiments in chapter \ref{ch:experiments}. The benchmark that was run on a single machine running both as client and server shows the ability to generate 1 million session requests per second from the client side, which generated a maximum of 20Gb/s of RAW TCP traffic from the server to the client. Two tests then need to be executed: a RAW TCP test between two different machines, and a test between client and server using HTTP. WARP is capable of talking HTTP, a layer 7 protocol. Monitoring of the amount of successful and failed session has to be done on both the client and server. When the API is used, detailed results can be retrieved as shown during the experimentation phase.   

\subsection{UC4 Application}
WARP is capable of talking HTTP. This use case requires a web server running on one of the machines. 
One possibility is to run WARP as a HTTP server, another possibility is to run an Apache or an NGINX server that provides a web page.
The client will use WARP to generate HTTP GET requests towards the server. The web server has to provide some files of different sizes. WARP can send a request for a certain file using a request size determined by the user (the content of the requests will be padded to generate the requested packetsize).  Resources of the server and stateful devices on the path towards the server will be claimed opening up the sessions. By sending a million requests per second the machines will be experiencing a Denial of Service (DOS) like attack.

\section{Real world scenario}
The use cases mentioned in section \ref{sec:usecase} have to be tested in a real world scenario to see if they are useful. Therefore the infrastructure of a company was used to test this method, keeping in mind the guidance from section 4.1 on the use of production networks.
Figure \ref{fig:nikhefuva} shows the simplified infrastructure for the test between client and server. All tests performed where using generated traffic for 90 seconds with a gap of 30 seconds before the next tests started in the same category (so, 2 minutes pass from the start of one test till the start of a new test). The clients always initiated the sessions to the DTN. The DTN is connected to a Data Center Interconnect (DCI) switch where an overlay technique is used to guarantee uptime of the data centers. 

\begin{figure}
  \includegraphics[scale=0.6]{images/nikhefuva.pdf}
  \caption{Infrastructure of the real world scenario for Use Case 4. The DTN on the left-hand side acts as the HTTP server.}
  \label{fig:nikhefuva}
\end{figure}

\subsection{UC1 Bandwidth Generation}
Using pktgen on top of DPDK with a packet size of 64 bytes generated a maximum of 42 Mpps towards the server. Pktgen only sends from one source to one destination, also using just one source and destination port. The link from the switch to the router at the client side is an aggregated link built up from 4x 10Gb/s interfaces. Hashing algorithms decide what physical interface is used per stream. Since the combination is always the same and a switch's default hashing algorithm uses layer 2 addresses as input, the packets followed the same path, causing all traffic to end up in a single 10Gb/s link towards the destination. The packet size is increased after 2 minutes from 64 to 400 bytes. This is the 'sweet spot' for the amount of packets and bits traveling over the line towards the destination with the potential of sending 11Mpps and 39Gb/s of traffic to the server.  

\subsection{UC2 Throughput}
Due to the hashing settings that cannot be changed without effect on the total environment, the result of use case 2 is predictable (based on the hashing problem observed during the use case 1) and will be 10Gb/s (capacity of the single link used). 40Gb/s cannot be reached.

\subsection{UC3 TCP Based}
Sending RAW TCP sessions between 2 servers using WARP with different packet sizes should provide a good representation of the capabilities of the servers and the intermediate devices in the path from source to destination. 
The server should be running WARP, acting as a TCP RAW server listening on 100 ports, and sending responses of a specified size when requests come in. The client should be configured to use 40 thousand ports for the requests, targeting the 100 ports at the server side. This makes a total of 4 million possible flows between client and server. Request and response sizes can be the same at every run. The request/response sizes depend on the link speed, in this case proper values are, 64, 256, 512, 1024 and 2048 bytes since utilization of the link is only 10\% when a packet size of 256 bytes is used.  

\subsection{UC4 Application}
%A web server needs to be setup offering a couple of different files. The files must be hosted in memory (on a RAM disk) and cashing was disabled to make sure disk IO will not become the bottleneck for the tests. Using WARP at the client side requesting the files from the DTN. NGINX\cite{nginx} ,the web server used for this test, was capable of delivering 11Gb/s inside the experimentation environment. This test will show the limits of the infrastructure towards the DTN or from the DTN itself running the application. 

A web server needs to be setup offering a couple of different files. The files must be hosted in memory (on a RAM disk) to make sure disk IO will not become the bottleneck for the tests. Cashing was disabled to make sure the performance of the web service was tested. Using WARP at the client side requesting the files from the DTN. NGINX\cite{nginx} was used as the web server for this test. This test will show the limits of the infrastructure towards the DTN or from the DTN itself running the application. WARP can be used as a web server responding with an HTTP 200-(OK) message, padded to make the Ethernet frame get the response size as configured. Testing application layer protocols using WARP can result in more bandwidth and more sessions per second which will stress the intermediate hardware to their limits. When the NGINX application test was executed without any problems WARP can be used to look for the limits of the intermediate hardware.

\

%What will be tested?

%Describe the test environment and the upcoming tests
%Why are these test useful? 
%What results are we looking for?
%What are the values/specific settings for these tests?
%What will be monitored on the receiver or sender side.
%Why am I monitoring this.
%What conclusion would I like to draw out of the test results.
 

%TEST environment. 
%2 identical machines running different operating systems at first.
%After preliminary tests the OS might be changed to one OS for final tests.
%In the middle sits a Juniper QFX10k2 as a router/switch.

%Limitations known inside the PCI buss connecting the card. Limited to 40 million packets per second.




%Ubuntu:
%./pktgen_sample03_burst_single_flow.sh -i eth2 -m 3c:8a:b0:34:2f:f0 -d 10.10.10.10 -s 9000
%-i = interface
%-m = destination mac
%-d = destination address
%-s = size
%-t = threads

%FreeBSD:
%./pkt-gen -i ixl0 -f tx -d 10.10.10.20 -s 10.10.10.10 -S 68:05:ca:32:17:e0 -l 9000
%-i = interface
%-f = function
%-d = destination ip addres
%-s = source ip address
%-S = source mac address
%-l = length
%-R = Rate (amount of packets per second)



