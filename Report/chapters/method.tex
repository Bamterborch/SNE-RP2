\chapter{Methodology}\label{ch:method}

In order to create a framework to get the limitations of the hardware and connecting infrastructure up to layer 7, use cases are formulated.
Only when the use cases are clear and there is an insight in the expected outcome, the correct applications can be selected for the framework.
The expected outcome is derived from the experiments. 

\section{Use cases}\label{sec:usecase}

Table \ref{tab:usecases} displays four different use cases. All serving a different purpose.
Use case 1 is designed to get the hardware limits of the NIC inside a host. Use case 2 is to get the limits of the  switching and routing hardware of the DUT.
Use case 3 will get the client and server limits up to layer 4. To handle TCP sessions, resources need to be allocated at client and server. 
Use case 4 is designed to get the limits of an application running on top of a kernel.  
Use cases 1 and 2 will show local limitations. 
When executing use cases 3 and 4 from a client to a server over a corporate network, using a statefull firewall, load balancers, aggregated links and active-standby machines.They will show the weakest link in the infrastructure towards the server.  
It should be noted that monitoring is in important part for showing the weakest link. 

\begin{table}[]
\centering
\caption{Use cases}
\label{tab:usecases}
\begin{tabular}{|l|l|l|l|}
\hline
NR & Use case                        & DUT            & Goal                                                                                             \\ \hline
UC1  & \begin{tabular}[c]{@{}l@{}}Bandwidth \\ generation\end{tabular}       & Client         & \begin{tabular}[c]{@{}l@{}}The goal is to see if the client is capable \\ of filling up the link and to reach the \\ maximum amount of pps \end{tabular} \\ \hline
UC2  & Throughput                    & Switch/router  & \begin{tabular}[c]{@{}l@{}}Generate the maximum amount of data in 2 ways \\ to make sure the hardware is able to forward\\ at line rate\end{tabular} \\ \hline
UC3  & TCP based                     & Client/Server  & Get the hardware limitations of the systems  \\ \hline
UC4  & Application             & \begin{tabular}[c]{@{}l@{}}Server and \\intermediate devices\end{tabular}         & \begin{tabular}[c]{@{}l@{}}The clients will overload the server with \\ requests at application level\end{tabular}  \\ \hline
\end{tabular}
\end{table}

\subsection{UC1}
In paragraph \ref{par:pps} it is stated that a 40Gb/s card needs to be able to handle 58 million packets with a size of 88 byte to handle a unidirectional stream of 40Gb/s.
Using PKTGEN, a tool on top of DPDK, TCP packets with a minimum size of 64 bytes (without inter frame gap, preamble and VLAN tag) can be created. This should generate 58 Mpps. To reach the 40Gb/s, larger packets have to be created. When the packet size is set to 1500 bytes, there have to be 3.3 Mpps to fill up the link. When Jumbo frames are used only 555 thousand packets per second are needed to fill up the link. When using PKTGEN on top of DPDK numbers like these can be reached using TCP traffic. When DPDK is used, the kernel cannot communicate to the devices anymore. So traffic statics cannot be read from the kernel. Monitoring on the switch ports connecting the servers is needed. 

\subsection{UC2}
The backplane of a switch should be able to forward traffic from one port to another at line speed. This should also be possible when the traffic is routed from one VLAN to the other. Routers should be able to rewrite packets at line rate. To test this a client and a server need to be connected to two different ports of the switch in a different segment or VLAN a high load should be generated from the client to the server, preferably the maximum link speed. Monitoring the ports of the switch using SNMP should show the same input rate on port 1 compared to the output rate of port 2.    

\subsection{UC3}
To get the limitations of the servers hardware when TCP is used, the kernel has to be bypassed. Using DPDK and WARP17 will show the limits of the hardware under load. As seen in the experiments in chapter \ref{ch:experiments}. The benchmark that was ran on a single machine running as client and server show the ability to open 1 million session per second, generating a maximum of 20Gb/s of RAW TCP traffic. Two tests need to be executed. A RAW TCP test between two different machines, and a test between client and server using HTTP. WARP17 is capable of talking HTTP, a layer 7 protocol. This should be the second test. Monitoring of the amount of successful and failed session has to be done on the client and server. When the API is used, detailed results can be retrieved as shown during the experimentation phase.   

\subsection{UC4}
WARP17 is capable of talking HTTP. This use case requires a web server running on one of the machines (the DTN). The client will use WARP17 to generate HTTP GET requests towards the DTN. The web server has to provide some files of different sizes. Warp can send a request for a certain file with a specified request size. Resources of the server and statefull devices on the path towards the server will be claimed opening up the sessions, By sending a million requests per second the machines will be experiencing a Denial of Service (DOS) like attack.


\section{Real world scenario}
The use cases mentioned in section \ref{sec:usecase} have to be tested in a real world scenario to see if they are useful. Therefore the infrastructure of a company was used to test this method.
Figure, \ref{fig:nikhefuva} shows the simplified infrastructure for the test between client and server. All tests performed, generated traffic for 90 seconds with a gap of 30 seconds before the next tests started in the same category. According to RFC2544 \cite{rfc2544} throughput tests need to take at least 60 seconds. The clients always initiated the sessions to the DTN.

\begin{figure}
  \includegraphics[scale=0.6]{images/nikhefuva.pdf}
  \caption{Infrastructure of real world scenario.}
  \label{fig:nikhefuva}
\end{figure}

\subsection{UC1}
Using PKTGEN on top of DPDK with a packet size of 64bytes generated a maximum of 42 Mpps towards the server. PKTGEN only sends from one source to one destination, also using just one source and destination port. The link from the switch to the router at the client side is an aggregated link build up from 4x 10Gb/s interfaces. Hashing algorithms decide what interface is used per stream. Since the combination is always the same and a switch's default hashing algorithm uses layer 2 addresses as input. The packets followed the same path, what ended up in a single 10Gb/s link towards the destination. The packet size needs to be increased after 2 minutes to 400bytes. This is the sweet spot for the amount of packets and bits traveling over the line towards the destination with the potential of sending 11Mpps and 39Gb/s of traffic to the server.  

\subsection{UC2}
Due to the hashing settings that cannot be changed without effect on the total environment, use case 2 does not need to be performed. A total of 40Gb/s cannot be reached.

\subsection{UC3}
Sending RAW TCP sessions between 2 servers using WARP17 with different packet sizes should provide a good representation of the capabilities of the servers and the intermediate devices in the path from source to destination. 
The server should be running WARP17, acting as a TCP RAW server listening on 100 ports. Sending responses of a specified size when requests come in. The client could be configured to use 40 thousand ports for the requests, targeting the 100 ports at the servers side. This makes a total of 4 million possible flows between client and server. Request and response sizes can be the same at every run. The request/response sizes depend on the link speed, in this case proper values are, 64, 256, 512, 1024 and 2048 bytes. 

\subsection{UC4}
A web server needs to be setup offering a couple of files from a RAMdisk, this should be done to make sure disk IO will not become the bottleneck for the tests. Using WARP17 at the client side requesting the files from the DTN. NGINX was capable of delivering 11Gb/s inside the experimentation environment. This test will get the limits of the infrastructure towards the DTN or from the DTN itself running the application. 

\

%What will be tested?

%Describe the test environment and the upcoming tests
%Why are these test useful? 
%What results are we looking for?
%What are the values/specific settings for these tests?
%What will be monitored on the receiver or sender side.
%Why am I monitoring this.
%What conclusion would I like to draw out of the test results.
 

%TEST environment. 
%2 identical machines running different operating systems at first.
%After preliminary tests the OS might be changed to one OS for final tests.
%In the middle sits a Juniper QFX10k2 as a router/switch.

%Limitations known inside the PCI buss connecting the card. Limited to 40 million packets per second.




%Ubuntu:
%./pktgen_sample03_burst_single_flow.sh -i eth2 -m 3c:8a:b0:34:2f:f0 -d 10.10.10.10 -s 9000
%-i = interface
%-m = destination mac
%-d = destination address
%-s = size
%-t = threads

%FreeBSD:
%./pkt-gen -i ixl0 -f tx -d 10.10.10.20 -s 10.10.10.10 -S 68:05:ca:32:17:e0 -l 9000
%-i = interface
%-f = function
%-d = destination ip addres
%-s = source ip address
%-S = source mac address
%-l = length
%-R = Rate (amount of packets per second)



