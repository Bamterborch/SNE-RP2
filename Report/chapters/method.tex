\chapter{Methodology}\label{ch:method}
From the results out of the experiments executed in the previous chapter, the use of two of the DPDK tools reached hardware limits and have given us the means to go up to application layer testing.  
As this is the underlying research question: going 'beyond pure network infrastructure testing', this report formulates some tests using the most adequate tools from the experimental phase going up to application layer testing.
The tests can be performed using hardware supported by DPDK and on top of a kernel supported by DPDK. 

\section{Tests}\label{sec:tests}

Table \ref{tab:tests} displays four tests, all serving a specific purpose.
Test 1 is should be performed to find the hardware limits of the NIC and the host that generates the traffic. 
When one knows these limits, one can choose to use more sources to generate traffic if necessary.  
Test 2 is to find the limits of the  switching and routing hardware of a DUT in the path between the client and the server.
Test 3 will find the client and server limits up to layer 4. To handle TCP sessions, resources need to be allocated at client, server and other stateful devices if present. 
Test 4 is capable of finding the limits of an application running on top of a kernel. This is the application layer test.  

Tests 1 and 2 will show possible limitation at OSI layer 2 and 3. 
Tests 3 and 4 are meant to test the entire path from a client to a server over a corporate network, that might include a stateful firewall, aggregated links and active-standby machines. The tests will stress the weakest link in the infrastructure towards the server.  
It should be noted that monitoring is a necessary part of the test in order to determine which device in the path is the 'weakest'. 
And monitoring is necessary to find the threshold for a device before it starts to fail. 

\begin{table}[H]
\centering
\caption{Performance tests}
\label{tab:tests}
\begin{tabular}{|l|l|l|l|}
\hline
NR & Tests                        & DUT            & Goal                                                                                             \\ \hline
T1  & \begin{tabular}[c]{@{}l@{}}Bandwidth \\ generation\end{tabular}       & Client         & \begin{tabular}[c]{@{}l@{}}The goal is to see if the client is capable \\ of filling up the link and to reach the \\ maximum amount of pps \end{tabular} \\ \hline
T2  & Throughput                    & Switch/router  & \begin{tabular}[c]{@{}l@{}}Generate the maximum amount of data in 2 ways \\ to make sure the hardware is able to forward\\ at line rate\end{tabular} \\ \hline
T3  & TCP based                     & Client/Server  & Get the limitations of the systems regarding memory and CPU usage. \\ \hline
T4  & Application             & \begin{tabular}[c]{@{}l@{}}Server and \\intermediate devices\end{tabular}         & \begin{tabular}[c]{@{}l@{}}The clients will try to overload the \\ server with requests at application level\end{tabular}  \\ \hline
\end{tabular}
\end{table}

\subsection{T1 Bandwidth Generation}
In paragraph \ref{par:pps} it is stated that a 40Gb/s card needs to be able to handle 56 million packets with a size of 88 bytes to handle a unidirectional stream of 40Gb/s.
Using pktgen on top of DPDK, Ethernet frames with a minimum size of 64 bytes containing TCP traffic (without inter frame gap, preamble and VLAN tag) can be created. To reach the 40Gb/s, larger packets have to be created. When the packet size is set to 1500 bytes, there have to be 3.3 Mpps to saturate the link. When 9000 byte packets are used only 555 Kpps are needed to saturate the link. When using pktgen on top of DPDK numbers like these can be reached using TCP traffic. When DPDK is used, the kernel does not communicate with the devices anymore since the nature of DPDK is to bypass the kernel drivers because the interfaces are claimed by DPDK. Traffic statics cannot be read from the kernel. Monitoring on the switch ports connecting the servers is needed. 
For this test the clients should try to generate the maximum amount of packets per second using Ethernet frames of 64 bytes. 
Section \ref{sub:pktgen} explained that a 'sweet spot' of maximum throughput and 11 Mpps where send by the client. The second part of the tests is sending Ethernet frames using a size of 400 bytes.  

\subsection{T2 Throughput}
The backplane of a switch should be able to forward traffic from one port to another at line rate. This should also be possible when the traffic is routed from one VLAN to the other. Routers should be able to route packets at line rate. To test this a client and a server need to be connected to two different ports of the router in a different segment or VLAN and the links maximum capacity should be filled. Frames of 400 bytes or more should be generated from the client to the server and vise versa, preferably the maximum link speed. Monitoring the ports of the switch using SNMP should show the same input rate on port 1 compared to the output rate of port 2.    

\subsection{T3 TCP Based}
To determine the limitations of the server hardware when TCP is used, the kernel has to be bypassed. The combination of DPDK and WARP will find the limits of the hardware under load. As seen in the experiments in chapter \ref{ch:experiments}. The benchmark that was run on a single machine, running both as client and server, shows the ability to generate 1 million session requests per second from the client side, which generated a maximum of 20Gb/s of RAW TCP traffic from the server to the client. 
Two useful tests can be derived from the results at the experimentation section: a RAW TCP test between two different machines, and a test between client and server using the HTTP v1.1 protocol. 
WARP is capable of sending  HTTP GET requests and it is capable of responding with a 200-(OK) message. 
Monitoring of the amount of successful and failed session has to be done on both the client and server. When the API is used, detailed results can be retrieved as shown during the experimentation phase. Otherwise the maximum amount of sessions per second has to be throttled by the client.    

\subsection{T4 Application}
WARP is capable of generating HTTP GET requests at a high rate (1 million session per second). This test requires a web server running on the destination server. 
When a live web application server is running this server can be used to get performance data for the particular web service. 
One should keep in mind that this web server will be a DUT and, according to RFC 2544, it probably will go down.   
The client will use WARP to generate HTTP GET requests for a web page hosted by the destination server. The web server can provide some files of different sizes. WARP has to send a request for a certain file using a request size determined by the user (the content of the requests will be padded to generate the configured frame size).  Resources of the client, the web server and stateful devices on the path towards the server will be claimed opening up the sessions. By sending a million requests per second the machines will be experiencing a Denial of Service (DOS) like attack.

\section{Real world scenario}
The tests mentioned in section \ref{sec:tests} need to be performed in a real world scenario to see if they are really useful. 
Therefore the infrastructure of a company was used to test this method, keeping in mind the guidance from section 4.1 on the use of production networks.
Figure \ref{fig:nikhefuva} shows the simplified infrastructure for the test between client and server. All tests where generating and sending traffic for 90 seconds with a gap of 30 seconds before the next step of the test started in the same category (so, 2 minutes pass from the start of one test till the start of a new test). The clients always initiated the sessions to the server. The server is connected to a Data Center Interconnect (DCI) switch where an overlay technique is used to guarantee uptime of the data centers. 

\begin{figure}
  \includegraphics[scale=0.6]{images/nikhefuva.pdf}
  \caption{Simplified Infrastructure of the real world scenario for Use Case 4. The DTN on the left-hand side acts as the HTTP server.}
  \label{fig:nikhefuva}
\end{figure}

\subsection{T1 Bandwidth Generation}
\todo[inline]{add script for these tests to appendixes}
Using pktgen on top of DPDK with a packet size of 64 bytes generated a maximum of 42 Mpps towards the server. 
Pktgen only sends from one source to one destination, also using just one source and destination port. 
Appendix \ref{appendix:software} contains the script used for this test. The plan was to ramp up the frame size from 64 bytes to 400 bytes. Unfortunately the link from the switch to the router at the client side is an aggregated link built up from 4x 10Gb/s interfaces. 
Hashing algorithms \cite{hashing} decide what physical interface is used per stream. Since the combination of source and destination IP and ports is always the same and a switch's default hashing algorithm uses layer 2 addresses as input, the packets followed the same path, causing all traffic to end up in a single 10Gb/s link towards the destination as expected. 
To reach the network of the external company, the data had to flow over the production network at Nikhef. Hashing algorithms could not be altered during the tests since hashing algorithms effect every aggregated link on the device. 
Expecting this behavior makes this test less useful in this case. For engineers that do not expect this behavior, the test will show the limitation and an engineer can correct the hashing behavior if desired.

\subsection{T2 Throughput}
Due to the hashing settings that could not be changed without effect on the total production environment, the result of the second test is predictable and will be 10Gb/s (capacity of the single link used). 40Gb/s cannot be reached.

\subsection{T3 TCP Based}
Sending RAW TCP sessions between 2 servers using WARP with different packet sizes should provide a good representation of the capabilities of the servers and the intermediate devices in the path from source to destination. 
The server should be running WARP, acting as a TCP RAW server listening on 100 ports, and sending responses of a specified size when requests come in. The client should be configured to use 40 thousand ports for the requests, targeting the 100 ports at the server side. This makes a total of 4 million possible flows between client and server. Request and response sizes can be equal at every run. The request/response sizes depend on the links capacity, in this case proper values are, 64, 256, 512, 1024 and 2048 bytes since utilization of the link is only 10\% when a packet size of 256 bytes is used.  

Appendix \ref{appendix:software} displays the script that was used for this test. WARP was started using 8 CPU's and using all the memory. 
Server A from the experimental phase was used as the server side. Server B was used as the client generating the requests.

\subsection{T4 Application}
%A web server needs to be setup offering a couple of different files. The files must be hosted in memory (on a RAM disk) and cashing was disabled to make sure disk IO will not become the bottleneck for the tests. Using WARP at the client side requesting the files from the DTN. NGINX\cite{nginx} ,the web server used for this test, was capable of delivering 11Gb/s inside the experimentation environment. This test will show the limits of the infrastructure towards the DTN or from the DTN itself running the application. 

A web server needs to be setup offering a couple of different files. The files must be hosted in memory (on a RAM disk) to make sure disk IO will not become the bottleneck for the tests. Cashing was disabled to make sure the performance of the web service was tested. Using WARP at the client side requesting the files from the web server. NGINX\cite{nginx} was used as the web service for this test. The performance tuning page from NGINX was used to set the correct settings for the tests\cite{nginxtuning}. The amount of sessions from a single clients was set to 50 thousand. This test will show the limits of the infrastructure towards the DTN or from the DTN itself running the web application. WARP can be used as a web server responding with an HTTP 200-(OK) message, padded to make the Ethernet frame get the response size as configured. Testing application layer protocols using WARP as a client can result in more bandwidth and more sessions per second then ever will happen in a real life scenario. Pushing the amount of sessions up could stress the intermediate hardware to their limits.

\
