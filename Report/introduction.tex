\chapter{Introduction}\label{ch:intro}

This paper addresses and describes the path towards obtaining knowledge on the limitations of hardware in the path towards a service, whilst accounting the full path from client to application (Open Systems Interconnect (OSI) layer 7). 
The research considers and explains the importance of the end-to-end user experience

Before an IT department offers a new systems or an application to the users, system and network administrators need to know the limitations of the hardware in the path towards an application in order to set thresholds for monitoring alerts. 
Routers and switches are capable of forwarding packets at line rate, making sure the server running an applications receives all the data destined for the application without unnecessary delays. 
To provide more bandwidth to an application, link aggregation is used to bundle multiple physical links to one logical link.

Generating high bandwidth to saturate a link using a session based protocol is possible. 
Kernel based open source tooling capable of generating data to saturate links well beyond 100Gb/s using UDP traffic, are available.  
Tools like iPerf\cite{iperf}, hping\cite{hping} and BoNeSi\cite{bonesi}i are kernel based and can be used to perform throughput tests up to OSI layer 3.  
%The needs for utilizing TCP to transfer data reliably and the increasing demand for bandwidth can result in capacity problems for both network infrastructures and applications. 

A path towards an application could contain stateful devices like a firewall or a load balancer. 
Stateful devices keep track of sessions, which evidently costs resources. 
These stateful devices in a path towards an application can become a bottleneck when their resources run out and can cause unavailability of services.
Alerting before resources run out is needed to keep a network and the services available.  
Therefore, end-to-end performance tests are needed from a client to a service in order to find the limitations in the infrastructure providing the service.

Institutes like Nikhef (a tier 1 location for the large Hadron Collider (LHC) Computing Grid) need to transfer large datasets from CERN. 
CERN produces around 30 petabytes annually\cite{cerndata}, which makes high-capacity links critical for Nikhef's research purposes.
The Nikhef network is designed around a high-capacity core that contains only stateless switching and routing devices. 
Distributed storage systems are directly connected thereto (akin to Data Transfer Nodes (DTN) in a ScienceDMZ\cite{sciencedmz}).
Besides capacity, data integrity is important, and therefore preferably TCP should be used for data streams between Nikhef and CERN. 
New equipment and services implemented by the engineers of Nikhef need to be tested if they can handle the expected load up to OSI layer 7 (the application layer).

The use of high bandwidth links and a need for session based application layer testing requires a different approach for traffic generation over network paths.

The Data Plane Development Kit\cite{dpdk} (DPDK) introduced by Intel offers a different approach for traffic generation, it does so without using the kernel network stack.  
Through DPDK, Linux userland applications are able to bypass the kernel and communicate with the network hardware directly. Memory, processors and interfaces have to be dedicated to DPDK.
Applications are then built on top of DPDK, utilizing DPDK's functionality to bypass the kernel (and the copying of memory regions inherent in such use). 
MoonGen\cite{moongen}, pktgen\cite{pktgen-dpdk}, and WARP\cite{warp} are designed based on different ideas offering the ability to test up to layer 7 of the OSI model.

%\section{Document layout}\label{sec:layout}
%The structure of this document is as follows: The problem is described in chapter \ref{ch:problem} which is followed by the research question from which this research project originated. To explain what research has been performed in this area chapter \ref{ch:related} shows the related work and provides a reason why this research was needed. The experimental setup is described in chapter \ref{ch:experiments}, this setup was used to evaluate what a selected set of tools had to offer as well as their limitations.
%Chapter \ref{ch:method} explains the usability of the tools and describes some tests that could be conducted using the tools. The tests where carried out on an environment that was planned to go into production - provided the tests described here demonstrated stable and scalable operation. All tests where conducted during a scheduled maintenance windows with approval of the owner of the environment. The results of the real world test can be found in chapter \ref{ch:results}. 
%From the results, conclusions are drawn in chapter \ref{ch:conclusion}. Appendix \ref{appendix:software} shows the test scripts used during tests on production infrastructures.

\section{Terminology and abbreviations}\label{sec:terminology}
The terminology used in this paper is based on RFC1242 \cite{rfc1242}, with the most relevant terms listed in table \ref{table:terms} for convenience.
A list of acronyms used in this paper can be found in appendix \ref{appendix:acronym}.

\begin{table}[]
\centering
\caption{Useful terminology}
\label{table:terms}
\begin{tabular}{|c|l|}
\hline
\textbf{Term}                  & \textbf{Explanation}                                                                                                                                                                                 \\ \hline
Constant Load         & Fixed length frames at a fixed interval time.                                                                                                                                                                    \\ \hline
Data link frame size  & \begin{tabular}[c]{@{}l@{}}The number of octets in the frame from the first octet \\ following the preamble to the end of the Frame check \\ Sequence (FCS), if present, or to the last octet of the data if \\ there is no FCS.\end{tabular} \\ \hline
Inter Frame Gap       & \begin{tabular}[c]{@{}l@{}}The delay from the end of a data link frame, \\ to the start of the preamble of the next data link frame.\end{tabular}                                                                \\ \hline
Overload behavior   & When demand exceeds available system resources.                                                                                                                                                                  \\ \hline
Throughput            & \begin{tabular}[c]{@{}l@{}}The maximum rate at which none of the offered frames, are \\ dropped by the device. \end{tabular}                                                                                                                                  \\ \hline 
\end{tabular}
\end{table}


